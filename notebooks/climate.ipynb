{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import string\n",
    "from math import pi\n",
    "from matplotlib import colors\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best places to live and work in the U.S. : Climate\n",
    "This notebook represents the \"Climate\" section of a larger project.\n",
    "\n",
    "I'm in my second year of undergrad, so naturally I've been thinking a lot about where to live once I graduate. In this project, I will explore data to augment my decision making. This project was heavily inspired by the list of best places to live by [U.S. Real Estate News](https://realestate.usnews.com/places/rankings/best-places-to-live). Their list is very compelling, and I was curious about their methods. I also found it difficult to compare two cities. So I decided to jump in myself!\n",
    "\n",
    "Before we get started, I want to mention [weatherspark](https://weatherspark.com/), which is a website that I found while working on this project. They probably have the best climate visualizations out there, so go check it out! \n",
    "\n",
    "## Problem definition\n",
    "What is the profile of each major city in the U.S.? I'll look at climate, population, and economy, as well as other factors that might be subjective and hard to measure (such as recreation, culture, religiosity, sports-ness, and political distribution). The focus of this experiment, compared to the U.S Real Estate News list is:\n",
    "* Make it easier to compare two or more cities against any parameter or a set of parameters\n",
    "* Consider how things are changing (e.g. 'What will the cost of living be in 10 years?' instead of 'What is the cost of living now?')\n",
    "* Investigate other factors that are not covered by U.S. Real Estate News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data & cleaning/processing\n",
    "\n",
    "### Cities\n",
    "As a starting point, we need a list of cities. The list of metropolitan areas (hitherto referred to as cities) used in this experiment was obtained from [Wikipedia](https://en.wikipedia.org/wiki/List_of_metropolitan_statistical_areas) and downloaded using [wikitable2csv](https://wikitable2csv.ggor.de/). We'll be referring to this list for the rest of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.read_csv(\n",
    "    '../data/raw/metros.csv',\n",
    "    usecols=[1, 2], # for climate analysis, let's just pull out the city name and the population\n",
    "    header=0,\n",
    "    names=['Metro', 'Pop']\n",
    ")\n",
    "cities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I (frustratingly) figured out that some of the metro descriptions actually have en dashes instead of hyphens. This caused some problems, so let's replace those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dashes(city):\n",
    "    return city['Metro'].replace(u'\\u2013', '-') \n",
    "\n",
    "cities['Metro'] = cities.apply(\n",
    "    lambda city: fix_dashes(city), # apply the function defined above\n",
    "    axis=1 # apply function row-wise\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate\n",
    "Climate data are from the [NOAA 1981-2010 normals](https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/climate-normals/1981-2010-normals-data). Several tables from the NOAA normals are used: temperature average, temperature min, temperature max, cloud cover, average windspeed, precipitation, and snowfall. An important thing to note is that the tables do not have column names. The data are labelled by weather station ID, which are inventoried in various files (there are different stations available for different varieties of metrics, e.g. not all stations collect hourly data). This means we're going to have to treat the tables as a relational database, using the station inventory as a lookup table. \n",
    "\n",
    "The values in the climate tables are 30-year normals (averages). Each value is actually accurate to one tenth of whatever the unit is (percent, degrees farenheit, inches, MPH, etc), but it is represented as an int. There's also a character that describes the nature of the calculation of the value, but we can disregard that for our purposes. So, 85.4 degrees farenheit will look like `854C` in the tables. This means we're going to have to do some cleaning before we can analyze the data.\n",
    "\n",
    "In order to connect cities with weather stations, we'll need their latitude and longitude. I retrieved this information from [OpenDataSoft](https://public.opendatasoft.com/explore/dataset/1000-largest-us-cities-by-population-with-geographic-coordinates/table/?sort=-rank&dataChart=eyJxdWVyaWVzIjpbeyJjb25maWciOnsiZGF0YXNldCI6IjEwMDAtbGFyZ2VzdC11cy1jaXRpZXMtYnktcG9wdWxhdGlvbi13aXRoLWdlb2dyYXBoaWMtY29vcmRpbmF0ZXMiLCJvcHRpb25zIjp7InNvcnQiOiItcmFuayJ9fSwiY2hhcnRzIjpbeyJhbGlnbk1vbnRoIjp0cnVlLCJ0eXBlIjoiY29sdW1uIiwiZnVuYyI6IkFWRyIsInlBeGlzIjoicmFuayIsInNjaWVudGlmaWNEaXNwbGF5Ijp0cnVlLCJjb2xvciI6IiNGRjUxNUEifV0sInhBeGlzIjoiY2l0eSIsIm1heHBvaW50cyI6NTAsInNvcnQiOiIifV0sInRpbWVzY2FsZSI6IiIsImRpc3BsYXlMZWdlbmQiOnRydWUsImFsaWduTW9udGgiOnRydWV9). This dataset actually has all the same fields as the Wikipedia data as well, but this dataset is about a decade older so I want to keep the population and population growth data from Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_latlong = pd.read_csv(\n",
    "    '../data/raw/cities-lat-long.csv',\n",
    "    sep=';',\n",
    "    usecols=['City', 'State', 'Coordinates']\n",
    ")\n",
    "cities_latlong.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can join `cities` and `cities_latlong`. There is one intermediate step - we have to pull out the major city name from `cities.'Metropolitan statistical area'` to join it on `cities_latlong.'City'`. While we're at it, let's pull out the state and put it in its own column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def major_city(city):\n",
    "    \"\"\" Given a metro, return the predominant city name. \"\"\" \n",
    "    metro = city['Metro']\n",
    "    metro = metro[0: metro.find(',')]\n",
    "    term = metro.find('-')\n",
    "    if term == -1:\n",
    "        return metro\n",
    "    return metro[0:term]\n",
    "\n",
    "def state(city):\n",
    "    \"\"\" Given a metro, return the state abbreviation. \"\"\"\n",
    "    metro = city['Metro']\n",
    "    states = metro[metro.find(',') + 2:]\n",
    "    state = states[0:2]\n",
    "    if state == 'DC':\n",
    "        state = 'MD'\n",
    "    return state\n",
    "\n",
    "cities['City'] = cities.apply(lambda city: major_city(city), axis=1)\n",
    "cities['State'] = cities.apply(lambda city: state(city), axis=1)\n",
    "cities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we could try to join `cities_latlong` on `'City'`, but we would run into an issue. There are some cities that share a name. For example, Portland OR and Portland ME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities.duplicated(subset='City', keep=False).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that there are 49 entries using the same name as another city. This would create unexpected results if we joined `cities` with `cities_latlong` on `'City'`. It's tempting to just drop the duplicate rows to avoid the issue, but that leaves out some key cities like Portland ME. Here's another idea: let's change the value for `'City'` of the duplicate with the lower population to the second city in the `'Metropolitan Statistical Area'` column. That won't work either, because many of the problematic cities aren't actually a metro at all, e.g. Springfield MO. It seems like the only option is to somehow join based on the city name AND the state. Fortunately, `cities_latlong` has a column for state. Unfortunately, it's not the two-letter abbreviation that shows up in `cities`. We're going to have to find a way to map the abbreviations to the state names.\n",
    "\n",
    "I pulled a table from [this website](https://statetable.com/) that will help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_table = pd.read_csv(\n",
    "    '../data/raw/state-table.csv',\n",
    "    usecols=[1, 2],\n",
    "    header=0,\n",
    "    names=['State name', 'State abbreviation']\n",
    ")\n",
    "state_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = cities.merge(\n",
    "    state_table,\n",
    "    how='left',\n",
    "    left_on='State',\n",
    "    right_on='State abbreviation'\n",
    ")\n",
    "cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities['Duplicate'] = cities.duplicated(subset='City', keep='first') \n",
    "# the values are sorted by population, so keep='first' allows us to ignore the more populous city\n",
    "cities.loc[cities['Duplicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = cities.merge(cities_latlong, how='inner', left_on=['City', 'State name'], right_on=['City', 'State'])\n",
    "cities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our dataframe has lat/long coordinates as well. We should separate the latitude and longitude into 2 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities[['Latitude', 'Longitude']] = cities['Coordinates'].str.split(', ', 1, expand=True).astype(float)\n",
    "cities.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things tidy, let's remove all the columns that we don't need. I'm also going to remove the 'MSA' from the metro descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = cities.drop(['City', 'Coordinates', 'State_x', 'State_y', 'Duplicate'], axis=1)\n",
    "cities['Metro'] = cities.apply(lambda x: x['Metro'][:x['Metro'].find(' MSA')], axis=1)\n",
    "cities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to save this version of the table for other analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities.to_csv('../data/processed/cities.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of the climate investigation, we only need the metro, state abbreviation, lat, and lon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = cities.drop(['Pop', 'State name'], axis=1)\n",
    "cities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for analysis\n",
    "The last step before we can start our investigation is transforming the tables into something usable. The climate tables vary between monthly, daily, and hourly values and are labelled by weather station. I'd like to consolidate all the data into one format and label the data by cities instead of stations. There's also a fair bit of cleaning to take care of. So there are three things to do:\n",
    "1. **Transform data to a common format.** I think monthly numbers are a sufficient level of fidelity for my purposes, so I can just aggregate all hourly and daily values into monthly values. \n",
    "2. **Change labels from stations to cities.** This is a little bit trickier. We're going to have to match cities with stations by minimizing the euclidean distance, calculated from latitude and longitude. \n",
    "3. **Clean the data.** During my exploration of the data, I noticed that the NOAA uses negative numbers as NULL placeholders. Different numbers mean different things, but for our purposes, we can just turn them all into actual NULLs.\n",
    "\n",
    "The code for all this is pretty involved, and there are several tables to process, so I'm going to define a few functions to handle all this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_station(city, table, stations_file):\n",
    "    \"\"\" \n",
    "    Given a row for a city and the stations table filepath,\n",
    "    return the ID of the closest weather station. \n",
    "    \"\"\"\n",
    "    \n",
    "    metro = city['Metro']\n",
    "    lat, lon = city['Latitude'], city['Longitude']\n",
    "    state = city['State abbreviation']\n",
    "    if state == 'DC':\n",
    "        state = 'MD'\n",
    "    # FIXME: D.C. creates problems joining the metro table with the weather station table\n",
    "    #        maybe just hand-pick a weather station for D.C.?\n",
    "    stations = pd.read_csv(\n",
    "            stations_file,\n",
    "            sep='\\s+',\n",
    "            header=None,\n",
    "            names=['ID', 'LAT', 'LON', '_', 'STATE', 'NAME', '_', '_', '_', '_', '_', '_'],\n",
    "            usecols=['ID', 'LAT', 'LON', 'STATE']\n",
    "    )\n",
    "    stations = stations.merge(\n",
    "        table,\n",
    "        how='inner',\n",
    "        left_on='ID',\n",
    "        right_on='STATION'\n",
    "    )\n",
    "    state_stations = stations[stations['STATE'] == state]\n",
    "    state_stations['distances'] = state_stations.apply(\n",
    "        lambda row: (row['LAT'] - lat)**2 + (row['LON'] - lon)**2,\n",
    "        axis=1\n",
    "    )\n",
    "    return state_stations.loc[state_stations['distances'].idxmin()]['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mly_to_analysis(table, stations_file):\n",
    "    \"\"\"\n",
    "    Given a monthly climate table, return the table transformed\n",
    "    to the analysis format.\n",
    "    \"\"\"\n",
    "    # start with a copy of the cities table\n",
    "    result = cities.copy()\n",
    "    \n",
    "    # add a column for the matching weather station\n",
    "    result['Station'] = result.apply(lambda x: find_station(x, table, stations_file), axis=1)\n",
    "    \n",
    "    # the station found may not be in the table, because stations with too many NULLs \n",
    "    # will be dropped\n",
    "        \n",
    "    # join with climate data\n",
    "    result = result.merge(\n",
    "        table,\n",
    "        how='inner',\n",
    "        left_on='Station',\n",
    "        right_on='STATION',\n",
    "        copy=False\n",
    "    ).drop(['STATION'], axis=1)\n",
    "    \n",
    "    result['Average'] = result.apply(lambda x: x[5:].mean(), axis=1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def dly_to_analysis(table, stations_file):\n",
    "    \"\"\"\n",
    "    Given a daily climate table, return the table transformed\n",
    "    to the analysis format.\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    # start with a copy of the cities table\n",
    "    result = cities.copy()\n",
    "    \n",
    "    # add a column for the matching weather station\n",
    "    result['Station'] = result.apply(lambda x: find_station(x, table, stations_file), axis=1)\n",
    "    \n",
    "    # prepare aggregate column of climate data\n",
    "    table['Month average'] = table.apply(lambda x: x[2:].mean(), axis=1)\n",
    "    \n",
    "    # get the subset of the table that we're interested in\n",
    "    table = table[['STATION', 'MONTH', 'Month average']]\n",
    "    \n",
    "    # pivot so that each month is a column instead of a row, using the average for the value\n",
    "    table = table.pivot(\n",
    "        index='STATION',\n",
    "        columns='MONTH',\n",
    "        values='Month average'\n",
    "    )\n",
    "    \n",
    "    # join with climate data\n",
    "    result = result.merge(\n",
    "        table,\n",
    "        how='inner',\n",
    "        left_on='Station',\n",
    "        right_on='STATION',\n",
    "        copy=False\n",
    "    )\n",
    "    \n",
    "    result['Average'] = result.apply(lambda x: x[5:].mean(), axis=1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def hly_to_analysis(table, stations_file):\n",
    "    \"\"\"\n",
    "    Given an hourly climate table, return the table transformed\n",
    "    to the analysis format.\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    \n",
    "    # start with a copy of the cities table\n",
    "    result = cities.copy()\n",
    "\n",
    "    # add a column for the matching weather station\n",
    "    result['Station'] = result.apply(lambda x: find_station(x, table, stations_file), axis=1)\n",
    "\n",
    "    # prepare aggregate column of climate data\n",
    "    table['Day average'] = table.apply(lambda x: x[3:].mean(), axis=1)\n",
    "\n",
    "    # get the subset of the table that we're interested in\n",
    "    table = table[['STATION', 'MONTH', 'DAY', 'Day average']]\n",
    "\n",
    "    # aggregate day averages \n",
    "    table = table.groupby(by=['STATION', 'MONTH']).agg('mean').drop(['DAY'], axis=1).reset_index().pivot(\n",
    "        index='STATION',\n",
    "        columns='MONTH',\n",
    "        values='Day average'\n",
    "    )\n",
    "\n",
    "    # join with climate data\n",
    "    result = result.merge(\n",
    "        table,\n",
    "        how='inner',\n",
    "        left_on='Station',\n",
    "        right_on='STATION',\n",
    "        copy=False\n",
    "    )    \n",
    "    \n",
    "    result['Average'] = result.apply(lambda x: x[5:].mean(), axis=1)\n",
    "    \n",
    "    return result    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need a function to read an NOAA file and turn it into a dataframe, so we can pass that dataframe to the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map of integers to month descriptions\n",
    "months_map = {\n",
    "    1: 'January',\n",
    "    2: 'February',\n",
    "    3: 'March',\n",
    "    4: 'April',\n",
    "    5: 'May',\n",
    "    6: 'June',\n",
    "    7: 'July',\n",
    "    8: 'August',\n",
    "    9: 'September',\n",
    "    10: 'October',\n",
    "    11: 'November',\n",
    "    12: 'December'\n",
    "}\n",
    "\n",
    "def _clean(value, mag):\n",
    "    \"\"\"\n",
    "    Define the behavior to clean values;\n",
    "    to be used in read_climate_table.\n",
    "    \"\"\"\n",
    "    # remove letters\n",
    "    value = int(str(value).strip(string.ascii_letters)) / mag\n",
    "    \n",
    "    # treat missing values as NULLs\n",
    "    if (value == -9999 / mag or\n",
    "        value == -8888 / mag or\n",
    "        value == -7777 / mag or \n",
    "        value == -6666 / mag or\n",
    "        value == -5555 / mag):\n",
    "        value = None\n",
    "    return value\n",
    "\n",
    "def read_climate_table(file, inv, form='None'):\n",
    "    \"\"\" \n",
    "    Return a dataframe from a NOAA\n",
    "    given a filepath and a format, \n",
    "    either 'mly', 'dly', or 'hly'.\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = [\n",
    "        'STATION',\n",
    "        1, 2, 3, 4,\n",
    "        5, 6, 7, 8,\n",
    "        9, 10, 11, 12\n",
    "    ] # numbers are months\n",
    "    \n",
    "    # describes the column at which the climate measurements start\n",
    "    # preceding this column is station (and month (and day))\n",
    "    values_start = 1\n",
    "    \n",
    "    if form == 'dly':\n",
    "        cols = [\n",
    "            'STATION', 'MONTH',\n",
    "            1,2,3,4,5,6,7,8,9,10,\n",
    "            11,12,13,14,15,16,17,18,19,20,\n",
    "            21,22,23,24,25,26,27,28,29,30,\n",
    "            31\n",
    "        ] # numbers are days of the month\n",
    "        values_start = 2\n",
    "        \n",
    "    if form == 'hly':\n",
    "        cols = [\n",
    "            'STATION', 'MONTH', 'DAY',\n",
    "            1,2,3,4,5,6,7,8,\n",
    "            9,10,11,12,13,14,15,16,\n",
    "            17,18,19,20,21,22,23,24\n",
    "        ] # numbers are hours of the day\n",
    "        values_start = 3\n",
    "    \n",
    "    # read the table file\n",
    "    df = pd.read_table(\n",
    "        file,\n",
    "        sep='\\s+',\n",
    "        header=None,\n",
    "        names=cols\n",
    "    )\n",
    "    \n",
    "    # values are represented in tenths (or 100ths for the precipitation file)\n",
    "    if 'prcp' in file and 'ge' not in file:\n",
    "        mag = 100\n",
    "    else:\n",
    "        mag = 10\n",
    "        \n",
    "    # apply _clean to all values except weather station\n",
    "    df.iloc[:, values_start:] = df.iloc[:, values_start:].apply(\n",
    "        lambda x: x.apply(\n",
    "            lambda y: _clean(y, mag)\n",
    "        ), axis=1\n",
    "    )\n",
    "    \n",
    "    # drop stations with too many missing values\n",
    "    if form == 'dly':\n",
    "        df = df.dropna(thresh=4)\n",
    "    else:\n",
    "        df = df.dropna()\n",
    "         \n",
    "    # convert to analysis format\n",
    "    if form == 'mly':\n",
    "        df =  mly_to_analysis(df, inv)\n",
    "    elif form == 'dly':\n",
    "        df = dly_to_analysis(df, inv)\n",
    "    elif form == 'hly':\n",
    "        df = hly_to_analysis(df, inv)\n",
    "        \n",
    "    # lastly, rename months columns\n",
    "    return df.rename(mapper=months_map, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can read the NOAA data into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily high temperature\n",
    "temp_hi = read_climate_table(\n",
    "    '../data/raw/climate/dly-tmax-normal.txt',\n",
    "    '../data/raw/climate/temp-inventory.txt',\n",
    "    form='dly'\n",
    ")\n",
    "\n",
    "# daily low temperature\n",
    "temp_lo = read_climate_table(\n",
    "    '../data/raw/climate/dly-tmin-normal.txt',\n",
    "    '../data/raw/climate/temp-inventory.txt',\n",
    "    form='dly'\n",
    ")\n",
    "\n",
    "# monthly rainfall in inches\n",
    "rain = read_climate_table(\n",
    "    '../data/raw/climate/mly-prcp-normal.txt',\n",
    "    '../data/raw/climate/prcp-inventory.txt',\n",
    "    form='mly'\n",
    ")\n",
    "\n",
    "# daily probability of 1 or more inches of rain\n",
    "# rain_prob = read_climate_table(\n",
    "#     '../data/raw/climate/dly-prcp-pctall-ge100hi.txt',\n",
    "#     '../data/raw/climate/prcp-inventory.txt',\n",
    "#     form='dly'\n",
    "# )\n",
    "\n",
    "# monthly snowfall in inches\n",
    "snow = read_climate_table(\n",
    "    '../data/raw/climate/mly-snow-normal.txt',\n",
    "    '../data/raw/climate/prcp-inventory.txt',\n",
    "    form='mly'\n",
    ")\n",
    "\n",
    "# daily probability of 1 or more inches of snow\n",
    "# snow_prob = read_climate_table(\n",
    "#     '../data/raw/climate/dly-snow-pctall-ge010ti.txt',\n",
    "#     '../data/raw/climate/hly-inventory.txt',\n",
    "#     form='dly'\n",
    "# )\n",
    "\n",
    "\n",
    "# hourly dewpoint temperature\n",
    "dewpoint = read_climate_table(\n",
    "    '../data/raw/climate/hly-dewp-normal.txt',\n",
    "    '../data/raw/climate/hly-inventory.txt',\n",
    "    form='hly'\n",
    ")\n",
    "\n",
    "\n",
    "# hourly overcast percentage\n",
    "clouds = read_climate_table(\n",
    "    '../data/raw/climate/hly-clod-pctovc.txt',\n",
    "    '../data/raw/climate/hly-inventory.txt',\n",
    "    form='hly'\n",
    ")\n",
    "\n",
    "\n",
    "# hourly windspeed\n",
    "wind = read_climate_table(\n",
    "    '../data/raw/climate/hly-wind-avgspd.txt',\n",
    "    '../data/raw/climate/hly-inventory.txt',\n",
    "    form='hly'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'm going to save each of these so I never have to worry about that ever again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_hi.to_csv('../data/processed/temp-hi.csv', index=False)\n",
    "temp_lo.to_csv('../data/processed/temp-lo.csv', index=False)\n",
    "rain.to_csv('../data/processed/rain.csv', index=False)\n",
    "snow.to_csv('../data/processed/snow.csv', index=False)\n",
    "dewpoint.to_csv('../data/processed/dewpoint.csv', index=False)\n",
    "clouds.to_csv('../data/processed/clouds.csv', index=False)\n",
    "wind.to_csv('../data/processed/wind.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in future runs, I can simply load these tables from the files I just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_hi = pd.read_csv('../data/processed/temp-hi.csv')\n",
    "# temp_lo = pd.read_csv('../data/processed/temp-lo.csv')\n",
    "# rain = pd.read_csv('../data/processed/rain.csv')\n",
    "# snow = pd.read_csv('../data/processed/snow.csv')\n",
    "# dewpoint = pd.read_csv('../data/processed/dewpoint.csv')\n",
    "# clouds = pd.read_csv('../data/processed/clouds.csv')\n",
    "# wind = pd.read_csv('../data/processed/wind.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation\n",
    "Now we're ready to do some investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping features\n",
    "Let's make some maps to help our brains process the climate data. We'll do this with GeoPandas.\n",
    "\n",
    "We'll need a shapefile map of the US. I'm using the states_21basic map from [ARC GIS](https://www.arcgis.com/home/item.html?id=f7f805eb65eb4ab787a0a3e1116ca7e5). I learned about how to use the map from [this article](https://medium.com/@erikgreenj/mapping-us-states-with-geopandas-made-simple-d7b6e66fa20d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "usa = gpd.read_file('../data/raw/maps/states_21basic/states.shp')\n",
    "\n",
    "# let's drop hawaii and alaska, because they make plotting very difficult\n",
    "contig_usa = usa.drop(\n",
    "    usa[usa['STATE_NAME'] == 'Hawaii'].index\n",
    ").drop(\n",
    "    usa[usa['STATE_NAME'] == 'Alaska'].index\n",
    ")\n",
    "\n",
    "#contig_usa will be passed to the plot function\n",
    "contig_usa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot the cities with geopandas, we'll have to turn the DataFrame into a GeoDataFrame. You can learn how to do this from the [GeoPandas documentation](https://geopandas.readthedocs.io/en/latest/gallery/create_geopandas_from_pandas.html). Learn about colormaps from the [matplotlib documentation](https://matplotlib.org/users/colormaps.html#colorcet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def climate_gdf(df=None):\n",
    "    \"\"\"\n",
    "    Return a GDF for geospatial plotting from a regular dataframe with latitude and longitude columns.\n",
    "    \"\"\"\n",
    "    df['Coordinates'] = list(zip(df['Longitude'], df['Latitude']))\n",
    "    df['Coordinates'] = df['Coordinates'].apply(Point)\n",
    "\n",
    "    # make the GDF\n",
    "    # exclude Alaska because it is hard to put on a map\n",
    "    gdf = gpd.GeoDataFrame(df[~(df['State abbreviation'] == 'AK')], geometry='Coordinates')\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to define a function that handles the mapping. I wanted to separate the data by month, so I'm using a widget to allow for month selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "def _climate_plot(gdf=None, month=1, cmap='Greens', base=None):\n",
    "    w = widgets.IntSlider(\n",
    "        value=1,\n",
    "        min=1,\n",
    "        max=12,\n",
    "        step=1,\n",
    "        description='Month:',\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation='horizontal',\n",
    "        readout=True,\n",
    "        readout_format='d'\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10,6), dpi=200)\n",
    "    sb.set(style='white')\n",
    "    ax.tick_params(\n",
    "        axis='both',\n",
    "        which='both',\n",
    "        bottom=False,\n",
    "        top=False,\n",
    "    )\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    base_plot = base.plot(color='0.6', edgecolor='white', ax=ax)\n",
    "    layer = gdf.plot(\n",
    "        markersize=20,\n",
    "        ax=ax,\n",
    "        alpha=1,\n",
    "        column=months_map[month],\n",
    "        cmap=cmap,\n",
    "        legend=True,\n",
    "        vmin=gdf.iloc[:,5:17].min().min(), vmax=gdf.iloc[:,5:17].max().max()\n",
    "    )\n",
    "    plt.title(months_map[month])\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def climate_plot(gdf=None, cmap='Greens', base=None):\n",
    "    interact_manual(\n",
    "        _climate_plot,\n",
    "        month=widgets.IntSlider(min=1,max=12,step=1,value=1),\n",
    "        gdf=fixed(gdf),\n",
    "        cmap=fixed(cmap),\n",
    "        base=fixed(base)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "climate_plot(\n",
    "    gdf=climate_gdf(df=clouds),\n",
    "    base=contig_usa,\n",
    "    cmap='viridis_r'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City profiling\n",
    "Let's try taking a look at individual cities. I currently live in Fort Collins, Colorado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "In order to use the climate evaluation as a parameter in this experiment, I think it would be best to come up with a climate classifier. Then we can add a new column to our data, `'Region'`, that will give us a good idea of what the weather is like there. \n",
    "\n",
    "The climate data are not labelled (i.e. there is no existing field `'Region'` describing cities that we could use to train a model), so we'll have to use an unsupervised learning technique. Let's try K means clustering from SciKit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def climate_normalize(df):\n",
    "    sub = df.iloc[:,5:18]\n",
    "    array = sub.values\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    array_scaled = scaler.fit_transform(array)\n",
    "    result = pd.DataFrame(\n",
    "        array_scaled,\n",
    "        columns=df.iloc[:,5:18].columns\n",
    "    )\n",
    "    result['Metro'] = df['Metro']\n",
    "    return result\n",
    "\n",
    "temp_hi_norm = climate_normalize(temp_hi)\n",
    "temp_lo_norm = climate_normalize(temp_lo)\n",
    "rain_norm = climate_normalize(rain)\n",
    "snow_norm = climate_normalize(snow)\n",
    "dewpoint_norm = climate_normalize(dewpoint)\n",
    "clouds_norm = climate_normalize(clouds)\n",
    "wind_norm = climate_normalize(wind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new df for clustering,\n",
    "# dropping all the columns except \n",
    "# for the features that we are going \n",
    "# to use for the model\n",
    "cities_m = cities.copy()\n",
    "\n",
    "cities_m = cities_m.merge(\n",
    "    temp_hi[['Metro', 'Average']],\n",
    "    how='inner', on='Metro'\n",
    ").rename(mapper={'Average': 'Temp high average'}, axis=1)\n",
    "cities_m = cities_m.merge(\n",
    "    temp_lo[['Metro', 'Average']],\n",
    "    how='inner', on='Metro'\n",
    ").rename({'Average': 'Temp low average'}, axis=1)\n",
    "cities_m = cities_m.merge(\n",
    "    rain[['Metro', 'Average']],\n",
    "    how='inner', on='Metro'\n",
    ").rename({'Average': 'Rainfall average'}, axis=1)\n",
    "cities_m = cities_m.merge(\n",
    "    snow[['Metro', 'Average']],\n",
    "    how='inner', on='Metro'\n",
    ").rename({'Average': 'Snowfall average'}, axis=1)\n",
    "cities_m = cities_m.merge(\n",
    "    dewpoint[['Metro', 'Average']],\n",
    "    how='inner', on='Metro'\n",
    ").rename({'Average': 'Dewpoint average'}, axis=1)\n",
    "cities_m = cities_m.merge(\n",
    "    clouds[['Metro', 'Average']],\n",
    "    how='inner', on='Metro'\n",
    ").rename({'Average': 'Overcast average'}, axis=1)\n",
    "cities_m = cities_m.merge(\n",
    "    wind[['Metro', 'Average']],\n",
    "    how='inner', on='Metro'\n",
    ").rename({'Average': 'Windspeed average'}, axis=1)\n",
    "\n",
    "cities_m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I wonder, should latitude and longitude be used in the model? This would encourage the climate classification to classify regions, not just similar weather patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can do any clustering, we have to normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = cities_m.iloc[:,4:]\n",
    "metros = cities_m['Metro']\n",
    "array = sub.values\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "array_scaled = scaler.fit_transform(array)\n",
    "cities_m = pd.DataFrame(\n",
    "    array_scaled,\n",
    "    columns=sub.columns\n",
    ")\n",
    "cities_m['Metro'] = metros\n",
    "cities_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.set(style='white')\n",
    "sb.pairplot(data=cities_m)\n",
    "fig = plt.gcf()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.savefig('../output/figures/pairplot.png', dpi=800, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8 # we have to specify the number of clusters\n",
    "model = KMeans(n_clusters=N) \n",
    "model.fit(cities_m.iloc[:,0:-1].values) # the model takes a NumPy array as input\n",
    "climate_clusters = model.predict(cities_m.iloc[:,0:-1].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of predict() is a NumPy 1D array containing the class for each row of the dataframe. This means we can just add this array to the dataframe as a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_m['Climate cluster'] = climate_clusters # add to the normalized array for evaluation later\n",
    "cities['Climate cluster'] = climate_clusters\n",
    "cities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got the code for this discrete color map function from [github.com/jakevdp](https://gist.github.com/jakevdp/91077b0cae40f8f8244a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_cmap(N, base_cmap=None):\n",
    "    \"\"\"Create an N-bin discrete colormap from the specified input map\"\"\"\n",
    "\n",
    "    base = plt.cm.get_cmap(base_cmap)\n",
    "    color_list = base(np.linspace(0, 1, N))\n",
    "    cmap_name = base.name + str(N)\n",
    "    if base_cmap == None:\n",
    "        return plt.cm.get_cmap(base_cmap, N)\n",
    "    return base.from_list(cmap_name, color_list, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6), dpi=200)\n",
    "sb.set(style='white')\n",
    "ax.tick_params(\n",
    "    axis='both',\n",
    "    which='both',\n",
    "    bottom=False,\n",
    "    top=False,\n",
    ")\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "base_plot = contig_usa.plot(color='0.7', edgecolor='w', ax=ax)\n",
    "layer = climate_gdf(cities).plot(\n",
    "    markersize=20,\n",
    "    ax=ax,\n",
    "    alpha=1,\n",
    "    column='Climate cluster',\n",
    "    cmap=discrete_cmap(N, base_cmap='gist_rainbow'),\n",
    "    legend=True\n",
    ")\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.savefig('../output/figures/clustermap.png', dpi=800, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an attempt to describe the climate clusters. To do so, we'll have to look at each cluster's distribution of each climate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sb.FacetGrid(cities_m, col='Climate cluster')\n",
    "g.map(sb.kdeplot, 'Temp high average')\n",
    "\n",
    "g = sb.FacetGrid(cities_m, col='Climate cluster')\n",
    "g.map(sb.kdeplot, 'Temp low average')\n",
    "\n",
    "g = sb.FacetGrid(cities_m, col='Climate cluster')\n",
    "g.map(sb.kdeplot, 'Rainfall average')\n",
    "\n",
    "g = sb.FacetGrid(cities_m, col='Climate cluster', ylim=100)\n",
    "g.map(sb.kdeplot, 'Snowfall average')\n",
    "\n",
    "g = sb.FacetGrid(cities_m, col='Climate cluster')\n",
    "g.map(sb.kdeplot, 'Dewpoint average')\n",
    "\n",
    "g = sb.FacetGrid(cities_m, col='Climate cluster')\n",
    "g.map(sb.kdeplot, 'Overcast average')\n",
    "\n",
    "g = sb.FacetGrid(cities_m, col='Climate cluster')\n",
    "g.map(sb.kdeplot, 'Windspeed average')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots describe the climate clusters. Let's create climate descriptors based on the distributions of climate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_map = {\n",
    "    0: 'warm/windy',\n",
    "    1: 'cool/cloudy/windy',\n",
    "    2: 'temperate/rainy/calm',\n",
    "    3: 'cool/dry',\n",
    "    4: 'hot/rainy/sticky',\n",
    "    5: 'cold/snowy/cloudy',\n",
    "    6: 'warm/sunny/calm',\n",
    "    7: 'cool/rainy/cloudy',\n",
    "}\n",
    "cities['Climate'] = cities.apply(lambda x: cluster_map[x['Climate cluster']], axis=1)\n",
    "cities = cities.drop(['Climate cluster'], axis=1)\n",
    "cities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking cities\n",
    "The whole point of this experiment is to figure out which cities have the best climate. We can do this by coming up with some ideal climate as a point in n-dimensional space, where n is the number of variables we've described. Then, we can rank cities in ascending order by their distance from this point. The cities closest to the point have the best climate.\n",
    "\n",
    "When we calculate the distance, we're going to want to use the normalized values. Otherwise, a difference along the `cloud` axis might be more significant than a difference along the `wind` axis, because the `cloud` values vary much more.\n",
    "\n",
    "The question is, what is the ideal climate? This is extremely subjective and even though I have a pretty good idea of weather that I like and don't like, I won't be able to come up with exact numbers to describe this. With all this in mind, I'll try my best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal = {\n",
    "    'temp_hi': 0.5,\n",
    "    'temp_lo': 0.4,\n",
    "    'rain': 0.7,\n",
    "    'dewpoint': 0.5,\n",
    "    'snow': 0.2,\n",
    "    'clouds': 0.5,\n",
    "    'wind': 0\n",
    "}\n",
    "\n",
    "def evaluate_climate(city, ideal):\n",
    "    \"\"\"\n",
    "    Given a city name and a dict describing the ideal climate,\n",
    "    return the distance of that city in n-dimensional \n",
    "    space from the point representing the ideal climate.\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    edges.append((ideal['temp_hi']  - temp_hi_norm[temp_hi_norm['Metro'] == city].iloc[0]['Average']) ** 2)\n",
    "    edges.append((ideal['temp_lo']  - temp_lo_norm[temp_lo_norm['Metro'] == city].iloc[0]['Average']) ** 2)\n",
    "    edges.append((ideal['rain']     - rain_norm[rain_norm['Metro'] == city].iloc[0]['Average']) ** 2)\n",
    "    edges.append((ideal['snow']     - snow_norm[snow_norm['Metro'] == city].iloc[0]['Average']) ** 2)\n",
    "    edges.append((ideal['dewpoint'] - dewpoint_norm[dewpoint_norm['Metro'] == city].iloc[0]['Average']) ** 2)\n",
    "    edges.append((ideal['clouds']   - clouds_norm[clouds_norm['Metro'] == city].iloc[0]['Average']) ** 2)\n",
    "    edges.append((ideal['wind']     - wind_norm[wind_norm['Metro'] == city].iloc[0]['Average']) ** 2)\n",
    "   \n",
    "    return sum(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cities['Score'] = cities.apply(lambda x: evaluate_climate(x['Metro'], ideal), axis=1)\n",
    "cities.head(50).sort_values('Score', ascending=True).head(10)['Metro']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to save a copy of this dataframe with the climate rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities.sort_values(\n",
    "    'Score', ascending=True\n",
    ").reset_index().rename(\n",
    "    mapper={'index': 'Climate rank'}, axis=1\n",
    ").drop(\n",
    "    ['Score', 'Coordinates'], axis=1\n",
    ").to_csv('../data/processed/climate-rankings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I'm going to map the climate scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6), dpi=200)\n",
    "sb.set(style='white')\n",
    "ax.tick_params(\n",
    "    axis='both',\n",
    "    which='both',\n",
    "    bottom=False,\n",
    "    top=False,\n",
    ")\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "base_plot = contig_usa.plot(color='0.7', edgecolor='white', ax=ax)\n",
    "layer = climate_gdf(cities).plot(markersize=20, ax=ax, alpha=1, column='Score', cmap='RdYlGn_r', legend=False)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.savefig('../output/figures/climate-rank-map.png', dpi=800, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
